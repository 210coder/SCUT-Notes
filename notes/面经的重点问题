你的项目是如何保证Redis的数据和Mysql数据一致性的问题的

1. 设置定期的过期时间

2. 手动删 delete key

3. mysql更新时把更新的部分写到redis里面



如何读取日志文件

1. 项目日志文件按照每日更新的变化 保存在每天的日志文件里
2. 通过开启线程不断读取每天的日志 不断循环从最后一次读的行read 当有新的日志行写入时 就会检测到变化
3. Kafka创建不断的日志消息主题 按照社交信息的类别 不断写入kafka 记录用户的系统行为 信息传播变化 后续用于日志的分析行为



高并发情况上如何解决

1. 部署多台服务器 集群管理 分流
2. 设置缓存 多级缓存
3. 用布隆过滤器查数据是否存在
4. 限制 访问权限 过滤掉一部分无效的访问
5. 设置分布式锁 



系统的用户级别



## 日志流收集系统项目

- **agent模块**：

每天产生一个日志文件，项目会把文件路径名更新到etcd里面，用一个线程去监听etcd 日志文件路径名的变化 有变化的时候 开一个线程（通过主线程去传日志文件路径的）去生产消息 线程需要判断这个文件有没有存在 没有的话分配错误  读取日志里面的变化日志 这个是通过什么去检测变化的 



加入读取日志的线程崩了 通过心跳检测重新分配线程 从上一次的offset去读



线程每次在读日志文件的时候定期保持日志文件的offset（这里有个问题 假如读取日志的时候没到时间记录offset  这个时候线程崩了 下一次线程启动的时候读取的时候从上一次offset读取 这样会有重复 这种不可避免 性能权衡的问题）



- **transfer模块 **

  线程数去消费消息 这个线程数量是和topic的分区数量是一致的 我的项目是创建一个线程去消费所有的topic消息



- 生产者和消费者的数量关系

kafka使用分区将topic的消息打散到多个分区分布保存在不同的broker上，

Kafka的producer和consumer都可以多线程地并行操作，而每个线程处理的是一个分区的数据。

对于producer而言，它实际上是用多个线程并发地向不同分区所在的broker发起Socket连接同时给这些分区发送消息；而consumer，同一个消费组内的所有consumer线程都被指定topic的某一个分区进行消费。



- Consumer个数与分区数有什么关系？

topic下的一个分区只能被同一个consumer group下的一个consumer线程来消费，但反之并不成立，即一个consumer线程可以消费多个分区的数据，比如Kafka提供的ConsoleConsumer，默认就只是一个线程来消费所有分区的数据。即分区数决定了同组消费者个数的上限

[link](https://www.jianshu.com/p/dbbca800f607)



Etcd作为 KV 存储，会为每个 key 都保留历史版本，比如用于发布回滚、配置历史等。

观察 key的变化:

```nginx
etcdctl watch  foo --rev=0
```











